对于简单的模型，我们可以直接求解析式，然后写出参数更新的公式。但是当层数变多之后，就难以写出解析式。
![[Pasted image 20240329135020.png]]
全连接神经网络中的一层如下：
![[Pasted image 20240329135403.png]]
![[Pasted image 20240329135409.png]]
这样就得到了前向传播的公式。展开后如下：
![[Pasted image 20240329135731.png]]
可见，如果只是线性运算，不管有多少层，最终都可以化简成1层。最终跟直接乘W矩阵一样了，那多层就没有任何意义。为了让公式无法化简，需要在层间增加一些非线性，让公式变得复杂，模型的表达能力也会提高。
![[Pasted image 20240329143021.png]]
反向传播过程，就是先输入数据进行前向传播，得到损失值，然后对损失函数求偏导，一步步反向传播，得到损失函数对于待训练变量的偏导。
![[Pasted image 20240329144848.png]]
## 在Pytorch中前馈和反馈
pytorch中，数据的数据存储在tensor里。它包含两个对象，一个是权重，一个是导数。
![[Pasted image 20240329145202.png]]
用法：
![[Pasted image 20240329145413.png]]
计算loss：
![[Pasted image 20240329145559.png]]
![[Pasted image 20240329150355.png]]
