直接使用c写代码效率不够高，使用汇编函数可以把一些反复被调用的函数占用的汇编指令简化，大幅提高运行速度。
```c
.text
.global add
.thumb //使用thumb指令集

add:
	add r0, r0, r1//ARPCS标准，第一个c调用asm函数时，第一个参数存入r0，第二个存入r1
	bx lr //将lr载入pc，lr中保存着函数调用地址，可以用来函数返回

```
## 内联汇编：

直接使用汇编有点麻烦，需要新建一个.s文件，可以直接使用内联汇编。
```c
int add(int a, int b)
{
	int sum;
	__asm__ volatile (
		"add %0, %1, %2" //汇编指令，每条之间用\n隔开
		:"=r"(sum); //输出操作数,格式是constain(c variablename)
		:"r"(a), "r"(b) //输入操作数,格式是constain(c variablename)
		:"cc" //表示该段代码会修改 flags reg
	);
	return sum;
}
```

其中volatile是**易变的**，告诉编译器不要优化代码，比如`mov r0, r0`，一般都加这个参数就可以。
其中变量的约束如下：
* m：表示传入有效的地址
* r：表示使用寄存器保存操作数
* i：传入立即数
* =：表示内联汇编会修改该操作数
* +：内联汇编会读写该操作数
* &：他是个early clobber（早期破坏）操作数（执行汇编代码时，输入操作数被早早的破坏了，后面的汇编代码本想使用原始的操作数，却读取到了被修改后的操作数，产生程序错误）。
## 原子变量
```c
atomic.h
typedef struct {
	int counter;
} atomic_t;
```
可见原子变量就是个普通变量。其特殊在操作函数。
**对于早于armv6架构的cpu，不支持多核，所以<span style="color:#ffc000">直接关中断</span>，等于晚于armv6架构的cpu，利用<span style="color:#ffc000">特殊寄存器</span>**

### <v6时操作的实现
宏展开，类似元编程
### >v6时操作的实现
 看内联汇编实现，主要分析add、ldrex、strex的作用




```c
static inline void atomic_add(int i, atomic_t *v)
{
	unsigned long tmp;
	int result;

	prefetchw(&v->counter);
	__asm__ __volatile__("@ atomic_add\n"
"1:	ldrex	%0, [%3]\n"
"	add	    %0, %0, %4\n"
"	strex	%1, %0, [%3]\n"
"	teq	    %1, #0\n"
"	bne	1b"	
	: "=&r" (result), "=&r" (tmp), "+Qo" (v->counter)
	: "r" (&v->counter), "Ir" (i)
	: "cc");
}	
```

`ldrex %0, [%3]`
* 读入
* 标记独占访问
`add %0, %0, %4`
* 修改
`strex %1, %0, [%3]` 
* 写入新值
	* 正常
		* 写入新值
		* 清除独占标记
		* ret=0
	* 被抢占（被别人抢先完成ldrex, strex。这时这个addr下的strex独占标记没了）
		* 发现非独占
		* 放弃写入
		* ret=1
		* 跳回开头，重新进行ldrex尝试，直到完成ldrex、strex
**v6之上这种方法，不需要关闭中断，<span style="color:#ffc000">对系统性能影响较小</span>**。

### 原子变量常规用法
```c
int my_drv_open()
{
	static atomic_t mydrv_atomic = ATOMIC_INIT(1);
	if(atomic_dec_and_test(&mydrv_atomic))
	{
		//测试-1结果为0，说明未被打断
		return 0;
	}
	//测试-1结果为-1，说明资源已经被占用，把原子变量复原吧
	atomic_inc(&my_drv_atomic);
	return -EBUSY;
}
```
# 自旋锁
* 对于多CPU系统，核A获取到了锁，这时核B则无法获取到锁，则会在临界区外循环等待。
* 对于单CPU系统，由于只有一个核，只存在进程间的抢占，所以可以直接禁止抢占即可。
* 如果linux内核版本比较老，本来就不支持内核态的抢占，那spin_lock就是空函数。 
* 如果不想被中断抢占，还要关掉中断。

**<span style="color:#ffc000">对于多CPU系统，自旋锁采用原子变量实现</span>**，但是有一点改进就是当有多个程序在获取一个锁时，会按照先后顺序来排队。
```c
typedef struct spinlock {
	union {
		struct raw_spinlock rlock;

#ifdef CONFIG_DEBUG_LOCK_ALLOC
# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))
		struct {
			u8 __padding[LOCK_PADSIZE];
			struct lockdep_map dep_map;
		};
#endif
	};
} spinlock_t;

typedef struct raw_spinlock {
	arch_spinlock_t raw_lock;
#ifdef CONFIG_DEBUG_SPINLOCK
	unsigned int magic, owner_cpu;
	void *owner;
#endif
#ifdef CONFIG_DEBUG_LOCK_ALLOC
	struct lockdep_map dep_map;
#endif
} raw_spinlock_t;

typedef struct {
	union {
		u32 slock;
		struct __raw_tickets {
#ifdef __ARMEB__
			u16 next;
			u16 owner;
#else
			u16 owner;
			u16 next;
#endif
		} tickets;
	};
} arch_spinlock_t;
```

可见在arch_spinlock_t中，存在着next和owner，来实现排队。
```c
static inline void arch_spin_lock(arch_spinlock_t *lock)
{
	unsigned long tmp;
	u32 newval;
	arch_spinlock_t lockval;

	prefetchw(&lock->slock);
	__asm__ __volatile__(
"1:	ldrex	%0, [%3]\n"
"	add	%1, %0, %4\n"   //lockval.next++
"	strex	%2, %1, [%3]\n"
"	teq	%2, #0\n"
"	bne	1b"
	: "=&r" (lockval), "=&r" (newval), "=&r" (tmp)
	: "r" (&lock->slock), "I" (1 << TICKET_SHIFT)
	: "cc");

	while (lockval.tickets.next != lockval.tickets.owner) {
		wfe();
		lockval.tickets.owner = READ_ONCE(lock->tickets.owner);
	}

	smp_mb();
}
```

```c
static inline void arch_spin_unlock(arch_spinlock_t *lock)
{
	smp_mb();
	lock->tickets.owner++; //不可能多个程序同时修改owner，所以不用加互斥措施 
	dsb_sev();
}
```
每个试图获取锁的进程，都会获取到属于自己的next，获取的越早，next就越小。当某个进程释放锁时，就会把owner++，然后下一个排队的进程owner\==next时，该进程便获取到了锁。
# 信号量
```c
struct semaphore {
	raw_spinlock_t		lock;
	unsigned int		count; //剩余资源
	struct list_head	wait_list; //如果剩余资源为0时，获取资源的进程被挂进等待队列，实现休眠
};
```
可见，信号量的实现依赖于自旋锁和等待队列。
```c
void down(struct semaphore *sem)
{
	unsigned long flags;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(sem->count > 0)) //如果资源存在，就获取资源然后返回
		sem->count--;
	else //资源没有
		__down(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}

static noinline void __sched __down(struct semaphore *sem)
{
	__down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);
}

static inline int __sched __down_common(struct semaphore *sem, long state,
								long timeout)
{
	struct semaphore_waiter waiter;

	list_add_tail(&waiter.list, &sem->wait_list);
	waiter.task = current;   //把当前进程放入等待队列
	waiter.up = false;

	for (;;) {
		if (signal_pending_state(state, current))
			goto interrupted;
		if (unlikely(timeout <= 0))
			goto timed_out;
		__set_current_state(state);  //修改进程状态为不可中断唤醒睡眠
		raw_spin_unlock_irq(&sem->lock); //释放自旋锁
		timeout = schedule_timeout(timeout); //启动调度
		raw_spin_lock_irq(&sem->lock);  //被唤醒后，获取spinlock
		if (waiter.up) //如果是因为获得了信号量而被唤醒，就返回，否则循环
			return 0;
	}

 timed_out:
	list_del(&waiter.list);
	return -ETIME;

 interrupted:
	list_del(&waiter.list);
	return -EINTR;
}
```


```c
void up(struct semaphore *sem)
{
	unsigned long flags;

	raw_spin_lock_irqsave(&sem->lock, flags);
	if (likely(list_empty(&sem->wait_list))) //如果没人等信号量，就count++
		sem->count++;
	else     //如果有人等信号量，就不用加了，直接唤醒这个进程
		__up(sem);
	raw_spin_unlock_irqrestore(&sem->lock, flags);
}

static noinline void __sched __up(struct semaphore *sem)
{
	struct semaphore_waiter *waiter = list_first_entry(&sem->wait_list,
						struct semaphore_waiter, list);
	list_del(&waiter->list); //从等待队列中删除该进程
	waiter->up = true;
	wake_up_process(waiter->task); //唤醒该进程
}
```
# 互斥量
mutex相当于count为1的信号量，但是内核针对mutex做了一些优化，使mutex比信号量效率更高。
```c
struct mutex {
	atomic_long_t		owner;
	spinlock_t		wait_lock;
	struct list_head	wait_list;
};
```

* mutex_lock
	* <span style="color:#ffc000">直接获得</span>
	* 休眠
* mutex_unlock
	* <span style="color:#ffc000">无人等待，直接释放</span>
	* 有人等待，休眠
<span style="color:#ffc000">fast_path</span>最有可能发生，slow_path偶尔发生
```c
void __sched mutex_lock(struct mutex *lock)
{
	might_sleep();

	if (!__mutex_trylock_fast(lock))
		__mutex_lock_slowpath(lock);
}
```
对于<span style="color:#ffc000">fast_path</span>
```c
static __always_inline bool __mutex_trylock_fast(struct mutex *lock)
{
	unsigned long curr = (unsigned long)current;
	unsigned long zero = 0UL;

	if (atomic_long_try_cmpxchg_acquire(&lock->owner, &zero, curr)) //判断lock->owner是否为0，如果为0，说明锁未被占用，则把curr赋值给owner，表明持有锁，然后返回
		return true;
		
	return false; //有进程正持有着锁，进入slow_path
}
```

对于slow_path：
```c
static noinline void __sched
__mutex_lock_slowpath(struct mutex *lock)
{
	__mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);
}

static int __sched
__mutex_lock(struct mutex *lock, long state, unsigned int subclass,
	     struct lockdep_map *nest_lock, unsigned long ip)
{
	return __mutex_lock_common(lock, state, subclass, nest_lock, ip, NULL, false);
}

static __always_inline int __sched
__mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
		    struct lockdep_map *nest_lock, unsigned long ip,
		    struct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)
{
	struct mutex_waiter waiter;
	bool first = false;
	struct ww_mutex *ww;
	int ret;

	might_sleep();

#ifdef CONFIG_DEBUG_MUTEXES
	DEBUG_LOCKS_WARN_ON(lock->magic != lock);
#endif

	ww = container_of(lock, struct ww_mutex, base);
	if (use_ww_ctx && ww_ctx) {
		if (unlikely(ww_ctx == READ_ONCE(ww->ctx)))
			return -EALREADY;

		/*
		 * Reset the wounded flag after a kill. No other process can
		 * race and wound us here since they can't have a valid owner
		 * pointer if we don't have any locks held.
		 */
		if (ww_ctx->acquired == 0)
			ww_ctx->wounded = 0;
	}

	preempt_disable();
	mutex_acquire_nest(&lock->dep_map, subclass, 0, nest_lock, ip);

	if (__mutex_trylock(lock) ||
	    mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, NULL)) {
		/* got the lock, yay! */
		lock_acquired(&lock->dep_map, ip);
		if (use_ww_ctx && ww_ctx)
			ww_mutex_set_context_fastpath(ww, ww_ctx);
		preempt_enable();
		return 0;
	}

	spin_lock(&lock->wait_lock);
	/*
	 * After waiting to acquire the wait_lock, try again.
	 */
	if (__mutex_trylock(lock)) {
		if (use_ww_ctx && ww_ctx)
			__ww_mutex_check_waiters(lock, ww_ctx);

		goto skip_wait;
	}

	debug_mutex_lock_common(lock, &waiter);

	lock_contended(&lock->dep_map, ip);

	if (!use_ww_ctx) {
		/* add waiting tasks to the end of the waitqueue (FIFO): */
		__mutex_add_waiter(lock, &waiter, &lock->wait_list);


#ifdef CONFIG_DEBUG_MUTEXES
		waiter.ww_ctx = MUTEX_POISON_WW_CTX;
#endif
	} else {
		/*
		 * Add in stamp order, waking up waiters that must kill
		 * themselves.
		 */
		ret = __ww_mutex_add_waiter(&waiter, lock, ww_ctx);
		if (ret)
			goto err_early_kill;

		waiter.ww_ctx = ww_ctx;
	}

	waiter.task = current;

	set_current_state(state);
	for (;;) {
		/*
		 * Once we hold wait_lock, we're serialized against
		 * mutex_unlock() handing the lock off to us, do a trylock
		 * before testing the error conditions to make sure we pick up
		 * the handoff.
		 */
		if (__mutex_trylock(lock))
			goto acquired;

		/*
		 * Check for signals and kill conditions while holding
		 * wait_lock. This ensures the lock cancellation is ordered
		 * against mutex_unlock() and wake-ups do not go missing.
		 */
		if (signal_pending_state(state, current)) {
			ret = -EINTR;
			goto err;
		}

		if (use_ww_ctx && ww_ctx) {
			ret = __ww_mutex_check_kill(lock, &waiter, ww_ctx);
			if (ret)
				goto err;
		}

		spin_unlock(&lock->wait_lock);
		schedule_preempt_disabled();

		/*
		 * ww_mutex needs to always recheck its position since its waiter
		 * list is not FIFO ordered.
		 */
		if ((use_ww_ctx && ww_ctx) || !first) {
			first = __mutex_waiter_is_first(lock, &waiter);
			if (first)
				__mutex_set_flag(lock, MUTEX_FLAG_HANDOFF);
		}

		set_current_state(state);
		/*
		 * Here we order against unlock; we must either see it change
		 * state back to RUNNING and fall through the next schedule(),
		 * or we must see its unlock and acquire.
		 */
		if (__mutex_trylock(lock) ||
		    (first && mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, &waiter)))
			break;

		spin_lock(&lock->wait_lock);
	}
	spin_lock(&lock->wait_lock);
acquired:
	__set_current_state(TASK_RUNNING);

	if (use_ww_ctx && ww_ctx) {
		/*
		 * Wound-Wait; we stole the lock (!first_waiter), check the
		 * waiters as anyone might want to wound us.
		 */
		if (!ww_ctx->is_wait_die &&
		    !__mutex_waiter_is_first(lock, &waiter))
			__ww_mutex_check_waiters(lock, ww_ctx);
	}

	mutex_remove_waiter(lock, &waiter, current);
	if (likely(list_empty(&lock->wait_list)))
		__mutex_clear_flag(lock, MUTEX_FLAGS);

	debug_mutex_free_waiter(&waiter);

skip_wait:
	/* got the lock - cleanup and rejoice! */
	lock_acquired(&lock->dep_map, ip);

	if (use_ww_ctx && ww_ctx)
		ww_mutex_lock_acquired(ww, ww_ctx);

	spin_unlock(&lock->wait_lock);
	preempt_enable();
	return 0;

err:
	__set_current_state(TASK_RUNNING);
	mutex_remove_waiter(lock, &waiter, current);
err_early_kill:
	spin_unlock(&lock->wait_lock);
	debug_mutex_free_waiter(&waiter);
	mutex_release(&lock->dep_map, ip);
	preempt_enable();
	return ret;
}
```
slow_path代码较多，对不在同一个CPU上锁进行了优化“<span style="color:#ffc000">乐观自旋</span>”，当其它CPU获取了锁正在运行时，会乐观地认为其很快就会释放锁，所以直接进行自选等待。
下面for循环中的内容，就跟信号量的实现很相似了，就是简单的判断count、加入等待队列等

对于mutex_unlock，也存在<span style="color:#ffc000">fast</span>和slow。<span style="color:#ffc000">fast</span>是指当前mutex+1\==0，这表示当前mutex无人等待，可以直接返回。如果+1\==0，则说明有人等待，这时进入slow。这就需要去等待队列，把它取出唤醒。